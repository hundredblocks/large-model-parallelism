![large-model-parallelism-illustration](https://user-images.githubusercontent.com/7229234/220430946-4b4ab497-7f86-40af-ac43-3e2793900bce.jpg)

# Model parallelism 101

Learn how model parallelism enables training models like stable diffusion and Chat GPT in less than 300 lines of code. This [notebook](https://github.com/hundredblocks/large-model-parallelism/blob/main/large-model-parallelism.ipynb) provides practical local implementations of the main model parallelism methods. It explores three approaches: data parallelism, tensor parallelism, and pipeline parallelism with a 2-layer MLP example that can be naturally extended to more complex models.

Reading this notebook will give you a solid overview of model parallelism techniques and an intuition for how to implement them.

Pull requests welcome. Illustration above generated with Lexica's Aperture model.
